{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeResult\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import PyPDF2\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relative paths from a given directory\n",
    "def get_files(directory, extension):\n",
    "    \"\"\"\n",
    "    Get a list of files in a directory with a given extension.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): The directory to search.\n",
    "    extension (str): The file extension to search for.\n",
    "\n",
    "    Returns:\n",
    "    A list of file paths.\n",
    "    \"\"\"\n",
    "\n",
    "    files = []\n",
    "    for root, dirs, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(extension):\n",
    "                files.append(os.path.relpath(os.path.join(root, filename), directory))\n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_azure_config(directory=\".\"):\n",
    "    filename = \"azure_config.yaml\"\n",
    "    filepath = directory + r\"/\" + filename\n",
    "    # Load Azure Config from azure_config.yaml\n",
    "    with open(filepath, \"r\") as stream:\n",
    "        try:\n",
    "            azure_config = yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:   \n",
    "            print(exc)\n",
    "    return azure_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the Azure Document Intelligence API\n",
    "def get_doc_client(AzureKeys):\n",
    "    # Set the values for the Azure Document Intelligence API\n",
    "    key = AzureKeys[\"DocumentIntelligence\"][\"KEY_1\"]\n",
    "    endpoint = AzureKeys[\"DocumentIntelligence\"][\"AZURE_ENDPOINT\"]\n",
    "    region = AzureKeys[\"DocumentIntelligence\"][\"AZURE_REGION\"]\n",
    "\n",
    "    # Create a client\n",
    "    client = DocumentIntelligenceClient(endpoint, AzureKeyCredential(key))\n",
    "    return client\n",
    "\n",
    "# Connect to Azure Blob Storage\n",
    "def get_blob_client(AzureKeys):\n",
    "    # Set the values for Azure Blob Storage\n",
    "    connection_string = AzureKeys[\"BlobStorage\"][\"CONNECTION_STRING\"]\n",
    "    container_name = AzureKeys[\"BlobStorage\"][\"CONTAINER_NAME\"]\n",
    "\n",
    "    # Create a BlobServiceClient\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    return container_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload document to blob storage\n",
    "def upload_blob(blob_client:BlobServiceClient, file_path:str, blob_name:str):\n",
    "    \"\"\"\n",
    "    Uploads a document to blob storage.\n",
    "\n",
    "    Parameters:\n",
    "    blob_client (BlobServiceClient): The blob service client.\n",
    "    file_path (str): The path to the document to upload.\n",
    "    blob_name (str): The name of the blob to create.\n",
    "\n",
    "    Returns:\n",
    "    The filename if upload fails.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_path, \"rb\") as data:\n",
    "        blob_client.upload_blob(name=blob_name, data=data)\n",
    "        \n",
    "        # Test if the blob was uploaded\n",
    "        blob = blob_client.get_blob_client(blob_name)\n",
    "\n",
    "        if(blob.exists()):  \n",
    "            print(\"Blob uploaded successfully.\")\n",
    "        else:\n",
    "            print(\"Blob not uploaded successfully.\")\n",
    "            return blob_name\n",
    "\n",
    "\n",
    "# Upload multiple documents to blob storage\n",
    "def upload_multiple_blobs(blob_client:BlobServiceClient, file_paths:list, blob_names:list):\n",
    "    \"\"\"\n",
    "    Uploads multiple documents to blob storage.\n",
    "\n",
    "    Parameters:\n",
    "        blob_client (BlobServiceClient): The blob service client.\n",
    "        file_paths (list): A list of paths to the documents to upload.\n",
    "        blob_names (list): A list of the names of the blobs to create.\n",
    "    \"\"\"\n",
    "    for file_path, blob_name in tqdm(zip(file_paths, blob_names), total=len(file_paths), desc=\"Uploading files to Azure Blob Storage\"):\n",
    "        upload_blob(blob_client, file_path, blob_name)\n",
    "\n",
    "def download_blob(blob_client, blob_name, file_path):\n",
    "    \"\"\"\n",
    "    Downloads a document from blob storage.\n",
    "        \n",
    "    Parameters:\n",
    "    blob_client (BlobServiceClient): The blob service client.\n",
    "    blob_name (str): The name of the blob to download.\n",
    "    file_path (str): The path to save the downloaded document.\n",
    "    \n",
    "    Returns:\n",
    "    A list of all files not downloaded.      \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_path, \"wb\") as data:\n",
    "        blob_client.download_blob(blob_name).readinto(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to open a PDF file and save each page as a separate PDF locally\n",
    "def split_pdf_file(load_file_path, save_directory_path):\n",
    "    \"\"\"\n",
    "    Splits a PDF file into separate pages and saves each page as a separate PDF file.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): The path to the PDF file to split.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the file name from the file path\n",
    "    file_name = load_file_path.split(r\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    # If save directory path does not exist, create it\n",
    "    if not os.path.exists(save_directory_path):\n",
    "        os.makedirs(save_directory_path)\n",
    "    \n",
    "    # Open the PDF file\n",
    "    pdf_file = open(load_file_path, \"rb\")\n",
    "    pdf = PyPDF2.PdfReader(pdf_file)\n",
    "    \n",
    "    # Save each page as a separate PDF file\n",
    "    for page_num in range(len(pdf.pages)):\n",
    "        pdf_writer = PyPDF2.PdfWriter()\n",
    "        pdf_writer.add_page(pdf.pages[page_num])\n",
    "        output_filename = f\"{save_directory_path}/{file_name}_page_{page_num+1}.pdf\"\n",
    "        with open(output_filename, \"wb\") as output_pdf:\n",
    "            # Write to output directory\n",
    "            pdf_writer.write(output_pdf)\n",
    "    \n",
    "        # Close the output PDF file\n",
    "        output_pdf.close()\n",
    "\n",
    "        # Close the PDF writer\n",
    "        pdf_writer.close()\n",
    "        \n",
    "        # Sleep for 0.5 second to avoid throttling\n",
    "        sleep(0.5)\n",
    "\n",
    "    # Close the PDF file\n",
    "    pdf_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pdf_files(file_paths: list, output_directory: str):\n",
    "    # Split each PDF file\n",
    "    for file in tqdm(file_paths, desc=\"Splitting PDF files...\", unit=\"file\"):\n",
    "        sleep(0.5)\n",
    "        split_pdf_file(file, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_document(filepath, doc_client, AzureKeys):\n",
    "    # Make sure your document's type is included in the list of document types the custom model can analyze\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        poller = doc_client.begin_analyze_document(\n",
    "            model_id=AzureKeys[\"DocumentIntelligence\"][\"MODEL_ID\"], analyze_request=f, content_type=\"application/octet-stream\"\n",
    "        )\n",
    "    doc_result: AnalyzeResult = poller.result()\n",
    "    return doc_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxonomy(taxonomy_dir=\".\"):\n",
    "    \"\"\"\n",
    "    Load the taxonomy from the taxonomy.yaml file.\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing the taxonomy.\n",
    "    \"\"\"\n",
    "\n",
    "    taxonomy_filename = \"taxonomy.yaml\"\n",
    "    taxonomy_filepath = taxonomy_dir + r\"/\" + taxonomy_filename\n",
    "\n",
    "    # Load taxonomy.yaml to dictionary\n",
    "    with open(taxonomy_filepath, \"r\") as stream:\n",
    "        try:\n",
    "            results = yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "\n",
    "    result_values = results['taxonomy']['fields']\n",
    "    schema = results['taxonomy']['schema']['results']\n",
    "\n",
    "    # Add the results schema to the result values\n",
    "    for field_key in result_values.keys():\n",
    "        result_values[field_key]['results'] = schema \n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def process_doc_result(doc_result, taxonomy_dict):\n",
    "    # print([key for key in doc_result['documents'][0]['fields'].keys()])\n",
    "\n",
    "    for key in doc_result.documents[0].fields.keys():\n",
    "        # Enter in doc model values\n",
    "        try:\n",
    "            data_type = doc_result['documents'][0]['fields'][key]['type']\n",
    "            data_field = \"value\" + data_type[0].upper() + data_type[1:]\n",
    "            taxonomy_dict['data']['taxonomy']['fields'][key]['results']['DOC_MODEL']['RAW_EXTRACTED_VALUE'] = \\\n",
    "                doc_result['documents'][0]['fields'][key][data_field]\n",
    "                \n",
    "            taxonomy_dict['data']['taxonomy']['fields'][key]['results']['DOC_MODEL']['CONFIDENCE'] = \\\n",
    "                doc_result['documents'][0]['fields'][key]['confidence']\n",
    "        except KeyError as e:\n",
    "            print(\"Key not detected: \" + str(e) + \" for key: \" + key + \" in doc model.\")\n",
    "\n",
    "    return taxonomy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_inference(file_paths, doc_client, AzureKeys):\n",
    "    # Submit all files in a list to Azure Document Intelligence\n",
    "    results = []\n",
    "    for file in tqdm(file_paths, desc=\"Analyzing documents...\", unit=\"file\"):\n",
    "        # load taxonomy to new dictionary\n",
    "        filename = file.split(r\"/\")[-1]\n",
    "        result = {'file': filename,\n",
    "                  'data': load_taxonomy() }\n",
    "        # Analyze the document\n",
    "        doc_result = analyze_document(file, doc_client, AzureKeys)\n",
    "        \n",
    "        # Process the document result\n",
    "        result = process_doc_result(doc_result, result)\n",
    "        # Append the result to the results list\n",
    "        results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing documents...:  14%|█▍        | 1/7 [00:07<00:42,  7.06s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key not detected: 'valueString' for key: ID_ISIN in doc model.\n",
      "Key not detected: 'valueString' for key: CURRENCY in doc model.\n",
      "Key not detected: 'valueString' for key: FREQUENCY in doc model.\n",
      "Key not detected: 'valueString' for key: TRADE_DATE in doc model.\n",
      "Key not detected: 'valueString' for key: INTEREST_TYPE in doc model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing documents...:  29%|██▊       | 2/7 [00:14<00:37,  7.47s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key not detected: 'INTEREST_TYPE' for key: INTEREST_TYPE in doc model.\n",
      "Key not detected: 'valueString' for key: ID_ISIN in doc model.\n",
      "Key not detected: 'valueString' for key: CURRENCY in doc model.\n",
      "Key not detected: 'valueString' for key: PERCENTAGE in doc model.\n",
      "Key not detected: 'valueString' for key: FREQUENCY in doc model.\n",
      "Key not detected: 'valueString' for key: TRADE_DATE in doc model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing documents...:  43%|████▎     | 3/7 [00:24<00:33,  8.43s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key not detected: 'INTEREST_TYPE' for key: INTEREST_TYPE in doc model.\n",
      "Key not detected: 'valueString' for key: ID_ISIN in doc model.\n",
      "Key not detected: 'valueString' for key: CURRENCY in doc model.\n",
      "Key not detected: 'valueString' for key: LISTING in doc model.\n",
      "Key not detected: 'valueString' for key: PERCENTAGE in doc model.\n",
      "Key not detected: 'valueString' for key: FREQUENCY in doc model.\n",
      "Key not detected: 'valueString' for key: TRADE_DATE in doc model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing documents...:  57%|█████▋    | 4/7 [00:31<00:23,  7.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key not detected: 'valueString' for key: ID_ISIN in doc model.\n",
      "Key not detected: 'valueString' for key: LISTING in doc model.\n",
      "Key not detected: 'valueString' for key: PERCENTAGE in doc model.\n",
      "Key not detected: 'valueString' for key: FREQUENCY in doc model.\n",
      "Key not detected: 'valueString' for key: TRADE_DATE in doc model.\n",
      "Key not detected: 'valueString' for key: INTEREST_TYPE in doc model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing documents...:  71%|███████▏  | 5/7 [01:01<00:31, 15.97s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key not detected: 'INTEREST_TYPE' for key: INTEREST_TYPE in doc model.\n",
      "Key not detected: 'valueString' for key: ID_ISIN in doc model.\n",
      "Key not detected: 'valueString' for key: LISTING in doc model.\n",
      "Key not detected: 'valueString' for key: PERCENTAGE in doc model.\n",
      "Key not detected: 'valueString' for key: FREQUENCY in doc model.\n",
      "Key not detected: 'valueString' for key: TRADE_DATE in doc model.\n",
      "Key not detected: 'valueDate' for key: MATURITY_DATE in doc model.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load Azure Keys\n",
    "    AzureKeys = load_azure_config()['AzureKeys']\n",
    "    AzureKeys\n",
    "\n",
    "    # Connect to Azure services\n",
    "    doc_client = get_doc_client(AzureKeys)\n",
    "    blob_client = get_blob_client(AzureKeys)\n",
    "\n",
    "\n",
    "    # Load data\n",
    "    data_path = \"./term sheets/source\"          # Source PDF directory\n",
    "    file_names = get_files(data_path, \".pdf\")   # Get all PDF files in the source data directory\n",
    "    file_paths = [data_path + \"/\" + file for file in file_names] # Get the full paths to the PDF files\n",
    "\n",
    "\n",
    "    # Initialize result as master data container\n",
    "    results = {}\n",
    "\n",
    "    # Perform AI Inference\n",
    "    results = ai_inference(file_paths, doc_client, AzureKeys)\n",
    "\n",
    "    results\n",
    " \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# MAIN function \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./term sheets/source/BBVA-Final-Terms-Series-164-Execution-version-1.pdf'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampledoc = [data_path + \"/\" + file for file in file_names][0]\n",
    "sampledoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./term sheets/source/BBVA-Final-Terms-Series-164-Execution-version-1.pdf']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Azure Keys\n",
    "AzureKeys = load_azure_config()['AzureKeys']\n",
    "AzureKeys\n",
    "\n",
    "# Connect to Azure services\n",
    "doc_client = get_doc_client(AzureKeys)\n",
    "blob_client = get_blob_client(AzureKeys)\n",
    "\n",
    "\n",
    "# Load data\n",
    "data_path = \"./term sheets/source\"          # Source PDF directory\n",
    "file_paths = get_files(data_path, \".pdf\")   # Get all PDF files in the source data directory\n",
    "file_names = [file.split(r\"/\")[-1] for file in file_paths]\n",
    "\n",
    "# Initialize result as master data container\n",
    "results = {}\n",
    "\n",
    "data_path = \"./term sheets/source\"\n",
    "filenames = get_files(data_path, \".pdf\")\n",
    "filepaths = [data_path + \"/\" + file for file in filenames]\n",
    "\n",
    "\n",
    "filepaths\n",
    "\n",
    "sample_paths = [filepaths[0]]\n",
    "sample_paths\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
